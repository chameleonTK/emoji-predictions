{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96de811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx.api import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0626b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.google.com/document/d/1yutoDtwenmmidc6H0ySu4BsZfpocS24LN6SRRYVZf-M/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8fc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4edd63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "raws = pd.read_csv(\"annotated_misp/unannotated_balanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9639839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "317d6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def dump_jsonl(output_path, data, append=False, progress=False):\n",
    "    \"\"\"\n",
    "    Write list of objects to a JSON lines file.\n",
    "    \"\"\"\n",
    "    mode = 'a+' if append else 'w'\n",
    "    with open(output_path, mode, encoding='utf-8') as f:\n",
    "        if progress:\n",
    "            data = tqdm(data)\n",
    "\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False)\n",
    "            f.write(json_record + '\\n')\n",
    "\n",
    "    print('Wrote {} records to {}'.format(len(data), output_path))\n",
    "\n",
    "def load_jsonl(input_path, verbose=True, progress=False) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        if progress:\n",
    "            f = tqdm(f)\n",
    "\n",
    "        for line in f:\n",
    "                data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "\n",
    "    if verbose:\n",
    "        print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d86b1b",
   "metadata": {},
   "source": [
    "## Read Annotated Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68169f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document('annotated_misp/annotated_balanced_p0.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ccad155",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = document.tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfc22d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythainlp\n",
    "# pythainlp.tokenize.word_tokenize(\"‡πÅ‡∏°‡∏ß‡∏Å‡∏¥‡∏ô‡∏õ‡∏•‡∏≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d7388eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "# emoji.replace_emoji('Python is üëç', replace='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a653c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a1a36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marisa_trie import Trie\n",
    "from pythainlp.corpus import get_corpus, thai_syllables, thai_words\n",
    "\n",
    "custom_dict = Trie([\"<number>\", \"<url>\", \"<user>\", \"<money>\", \"<time>\", \"<date>\", \"<phone>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0834f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1001/1001 [00:26<00:00, 37.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "data = []\n",
    "\n",
    "keys = None\n",
    "rows = []\n",
    "marked_raws = {}\n",
    "for i, row in tqdm(enumerate(table.rows), total=len(table.rows)):\n",
    "    if i==0:\n",
    "        continue\n",
    "    \n",
    "    s = \"\"\n",
    "    tags = []\n",
    "    spans = []\n",
    "    words = []\n",
    "    for cell in row.cells:\n",
    "        for para in cell.paragraphs:\n",
    "            for run in para.runs:\n",
    "                \n",
    "                span = emoji.replace_emoji(run.text, replace='')\n",
    "                ws = pythainlp.tokenize.word_tokenize(span.strip(), custom_dict=custom_dict)\n",
    "                \n",
    "                if \"<\" in ws and \">\" in ws:\n",
    "                    assert(False)\n",
    "                    \n",
    "                if run.bold:\n",
    "                    tags.append({\n",
    "                        \"span\": ws,\n",
    "                        \"tag\": \"int\",\n",
    "                        \"s\": len(s),\n",
    "                        \"t\": len(s)+len(span)\n",
    "                    })\n",
    "                \n",
    "                if run.italic:\n",
    "                    tags.append({\n",
    "                        \"span\": ws,\n",
    "                        \"tag\": \"msp\",\n",
    "                        \"s\": len(s),\n",
    "                        \"t\": len(s)+len(span)\n",
    "                    })\n",
    "                \n",
    "                if run.underline:\n",
    "                    tags.append({\n",
    "                        \"span\": ws,\n",
    "                        \"tag\": \"tran\",\n",
    "                        \"s\": len(s),\n",
    "                        \"t\": len(s)+len(span)\n",
    "                    })\n",
    "                \n",
    "                \n",
    "                words += ws\n",
    "                spans.append(ws)\n",
    "                s += span\n",
    "    \n",
    "    \n",
    "    raw_texts = raws.iloc[i-1].to_dict()\n",
    "    marked_raws[i-1] = True\n",
    "    \n",
    "    rows.append({\n",
    "        **raw_texts,\n",
    "        \"original\": raw_texts[\"text\"],\n",
    "        \"text\": s,\n",
    "        \"tags\": tags,\n",
    "        \"spans\": spans,\n",
    "        \"words\": words,\n",
    "        \"no_ws_words\": [w.strip() for w in words if len(w.strip())>=0],\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Validate with distance\n",
    "# import editdistance\n",
    "\n",
    "# for i, r in enumerate(rows):\n",
    "# #     print(i, r.keys())\n",
    "#     if (editdistance.eval(r[\"text\"], r[\"preprocessed\"])) > 10:\n",
    "#         print(r[\"text\"])\n",
    "#         print(r[\"preprocessed\"])\n",
    "#         print(\"===================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d7e8f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "187d0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:03, 3058.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "for i, row in tqdm(raws.iterrows()):\n",
    "    if i in marked_raws:\n",
    "        continue\n",
    "    \n",
    "    raw_texts = {**row.to_dict()}\n",
    "    \n",
    "    text = raw_texts[\"preprocessed\"]\n",
    "    span = emoji.replace_emoji(text, replace='')\n",
    "    ws = pythainlp.tokenize.word_tokenize(span.strip(), custom_dict=custom_dict)\n",
    "    \n",
    "    \n",
    "    tags = []\n",
    "    words = ws\n",
    "    spans = [ws]\n",
    "    s = span\n",
    "\n",
    "    train_data.append({\n",
    "        **raw_texts,\n",
    "        \"original\": raw_texts[\"text\"],\n",
    "        \"text\": s,\n",
    "        \"tags\": tags,\n",
    "        \"spans\": spans,\n",
    "        \"words\": words,\n",
    "        \"no_ws_words\": [w.strip() for w in words if len(w.strip())>=0],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cc5ad66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 9000)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data), len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6916677f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 9000 records to annotated_misp/train_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "dump_jsonl(\"annotated_misp/train_data.jsonl\", train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3d1eeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1000 records to annotated_misp/test_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "dump_jsonl(\"annotated_misp/test_data.jsonl\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ad9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
